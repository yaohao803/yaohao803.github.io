0
00:00:03,310 --> 00:00:07,580
Welcome to this course on compilers. My name is Alex Aiken. I'm a professor here

1
00:00:07,580 --> 00:00:11,539
at Stanford University. And we're going to be talking about the implementation of

2
00:00:11,539 --> 00:00:18,539
programming languages. >> There are two major approaches to implementing

3
00:00:19,619 --> 00:00:24,599
programming languages, compilers, and interpreters. Now, this class is mostly

4
00:00:24,599 --> 00:00:30,210
about compilers. But, I do want to say a few words about interpreters here in the

5
00:00:30,210 --> 00:00:36,250
first lecture. So, what does an interpreter do? Well, I'm gonna draw a

6
00:00:36,250 --> 00:00:41,240
picture here, this box is the interpreter, and it takes, let me label it with a big

7
00:00:41,240 --> 00:00:48,240
I, it takes as input, your program. That you wrote, And, whatever data that you

8
00:00:49,920 --> 00:00:56,920
want to run the program on. And it produces the output directly. Meaning that

9
00:00:56,920 --> 00:01:01,580
it doesn't do any processing of the program before it executes it on the

10
00:01:01,580 --> 00:01:06,470
input, So you just write the program, and you invoke the interpreter on the data,

11
00:01:06,470 --> 00:01:11,380
and the program immediately begins running. And so, we can say that the

12
00:01:11,380 --> 00:01:17,740
interpreter is, is online, meaning it the work that it does is all part of running

13
00:01:17,740 --> 00:01:23,950
your program. Now a compiler is structured differently. So, we can draw a picture

14
00:01:23,950 --> 00:01:30,799
here. Which we'll label with a big C, for the compiler, And the compiler takes as

15
00:01:30,799 --> 00:01:37,799
input just your program. And then it produces an executable. And this

16
00:01:40,619 --> 00:01:45,409
executable is another program, might be assembly language, it might be bytecode.

17
00:01:45,409 --> 00:01:50,069
It could be in any number of different implementation languages. But now this can

18
00:01:50,069 --> 00:01:57,069
be run separately on your data. And that will produce the output. Okay? And so in

19
00:02:03,369 --> 00:02:10,369
this structure the compiler is offline, Meaning that we pre-process the program

20
00:02:12,159 --> 00:02:16,680
first. The compiler is essentially a pre-processing step that produces the

21
00:02:16,680 --> 00:02:21,319
executable, and then we can run that same executable on many, many different inputs,

22
00:02:21,319 --> 00:02:26,540
on many different data sets without having to recompile or do any other processing of

23
00:02:26,540 --> 00:02:31,590
the program. I think it's helpful to give a little bit of history about how

24
00:02:31,590 --> 00:02:35,860
compilers and interpreters were first developed. So the story begins in the

25
00:02:35,860 --> 00:02:41,530
1950s and in particular with a machine called the 704 built by IBM. Thi s was

26
00:02:41,530 --> 00:02:45,030
their first commercially successful machine, although there had been some

27
00:02:45,030 --> 00:02:49,849
earlier machines that they had tried out. But anyway the interesting thing about the

28
00:02:49,849 --> 00:02:54,489
704 well, once customers started buying it and using it, is that they found that the

29
00:02:54,489 --> 00:03:00,439
software costs exceeded the hardware costs. And not just by a little bit, but

30
00:03:00,439 --> 00:03:06,079
by a lot And, This is important because these, the hardware in these, those days

31
00:03:06,079 --> 00:03:12,340
was extremely expensive. And even then when hardware cost the most in absolute

32
00:03:12,340 --> 00:03:16,909
and relative terms, more than they would ever cost again, already the software was

33
00:03:16,909 --> 00:03:23,200
the dominant expense in, in making good use out of computers. And this led a

34
00:03:23,200 --> 00:03:28,599
number of people to think about how they could do a better job of writing software.

35
00:03:28,599 --> 00:03:35,599
How they could make programming more productive. Where the earliest efforts to

36
00:03:36,069 --> 00:03:41,189
improve the productivity of programming was called speed coding, developed in 1953

37
00:03:41,189 --> 00:03:48,189
by John Backus. Now, speed coding is what we call today, an early example of an

38
00:03:48,299 --> 00:03:53,090
interpreter. And like all interpreters, it had some advantages and disadvantages. The

39
00:03:53,090 --> 00:03:57,680
primary advantage was that it was much faster, to develop the programs. So the,

40
00:03:57,680 --> 00:04:01,569
in that sense, the programmer was much more productive, But among its

41
00:04:01,569 --> 00:04:07,310
disadvantages, code written, speed code programs were ten to twenty times slower.

42
00:04:07,310 --> 00:04:11,909
Then handwritten programs and that's also true of interpreted programs today. So if

43
00:04:11,909 --> 00:04:14,890
you have an implementation that uses an interpreter, they're going to be much

44
00:04:14,890 --> 00:04:21,139
slower than either a compiler or writing code by hand. And also, the speed code

45
00:04:21,139 --> 00:04:26,970
interpreter took up, 300 bytes of memory. And that doesn't seem like very much. In

46
00:04:26,970 --> 00:04:32,259
fact, 300 bytes, today, would seem like an incredibly tiny, program. But in those

47
00:04:32,259 --> 00:04:37,199
days, you have to keep in mind, that this was 30 Percent Of the memory on the

48
00:04:37,199 --> 00:04:42,080
machine. So this was 30 percent of the entire memory of the 704. And so the

49
00:04:42,080 --> 00:04:47,470
amount of space that the interpreter took up was itself a concern. Now speed coding

50
00:04:47,470 --> 00:04:51,800
did not become popular, but John Backus thought it was promising and it gave him

51
00:04:51,800 --> 00:04:56,150
the idea for another project. The most important applications in those days were

52
00:04:56,150 --> 00:05:01,000
scientific computations, and programmers thought in terms of writing down formulas

53
00:05:01,000 --> 00:05:06,960
in a form that the machine could execute. John thought that the problem with speed

54
00:05:06,960 --> 00:05:11,650
coding was that the formulas were in fact interpreted and he thought if first the

55
00:05:11,650 --> 00:05:18,190
formulas were translated in to a form that the machine could execute directly. That

56
00:05:18,190 --> 00:05:23,949
the code would be faster, And while still allowing the programmer to write the, the,

57
00:05:23,949 --> 00:05:30,949
the programs at a high level, and thus was the Formula Translation Project or FORTRAN

58
00:05:31,370 --> 00:05:38,370
Project born. Now FORTRAN ran from 1954 To 1957, And interestingly, they thought it

59
00:05:40,280 --> 00:05:44,780
would only take them one year to build the compiler but it would end up taking three.

60
00:05:44,780 --> 00:05:49,300
So just like today, they weren't very good at predicting how long software projects

61
00:05:49,300 --> 00:05:56,190
would take. But it was a very successful project. By 1958, over 50 percent of all

62
00:05:56,190 --> 00:06:03,190
code was written in FORTRAN. So 50 percent of programs were in FORTRAN, And, that is

63
00:06:04,080 --> 00:06:08,940
very rapid adoption of a new technology. We would be happy with that kind of

64
00:06:08,940 --> 00:06:12,990
success today, and of course at that time they were ecstatic, And everybody thought

65
00:06:12,990 --> 00:06:16,430
that FORTRAN both raised the level of abstraction, improved programmer

66
00:06:16,430 --> 00:06:23,430
productivity, and allowed everyone to make much better use of these machines. So

67
00:06:24,129 --> 00:06:29,490
FORTRAN one was the first successful high level language and it had a huge impact on

68
00:06:29,490 --> 00:06:33,770
computer science. In particular, it led to an enormous body of theoretical work. And

69
00:06:33,770 --> 00:06:37,419
one of the interesting things about programming languages, actually, is the

70
00:06:37,419 --> 00:06:43,520
combination of theory. And practice because it's not really possible in

71
00:06:43,520 --> 00:06:49,000
programming languages to do a good job without having both a, a very good grasp

72
00:06:49,000 --> 00:06:52,699
of fairly deep theory and also good engineering skills. So there's a lot of

73
00:06:52,699 --> 00:06:56,639
very good systems building material in programming languages and typically it

74
00:06:56,639 --> 00:07:00,990
involves a very subtle and fruitful interaction with theory. And so, and this

75
00:07:00,990 --> 00:07:04,550
is one of the things, I think, that's most attractive about the area's the subject of

76
00:07:04,550 --> 00:07:09,830
studying computer science. And the impact of FORTRAN was not just on computer

77
00:07:09,830 --> 00:07:15,259
science research, of course, but also on the development of, practical compilers.

78
00:07:15,259 --> 00:07:20,699
And, in fact, its influence was so profound, that today, auto compilers still

79
00:07:20,699 --> 00:07:27,669
preserve the outlines of FORTRAN one. So what was the structure of FORTRAN one?

80
00:07:27,669 --> 00:07:32,470
Well it consists five phases lexical analysis and parsing, which together take

81
00:07:32,470 --> 00:07:38,300
care of the syntactic aspects of the language, semantic analysis, which, of

82
00:07:38,300 --> 00:07:43,300
course, takes care of more semantic aspects, things like types and scope

83
00:07:43,300 --> 00:07:50,300
rules. Optimization, Which is a collection of transformations on the program to

84
00:07:50,800 --> 00:07:55,939
either make it run faster or use less memory. And finally code generation which

85
00:07:55,939 --> 00:08:00,970
actually does the translation to another generation. And depending on our goals,

86
00:08:00,970 --> 00:08:06,229
that translation might be to machine codes. It might be to a bite code for a

87
00:08:06,229 --> 00:08:12,069
virtual machine. It might be to another high level programming language. Well

88
00:08:12,069 --> 00:08:16,139
that's it for this lecture, and next time we'll pick up here and talk about these

89
00:08:16,139 --> 00:08:17,999
five phases in more detail.
