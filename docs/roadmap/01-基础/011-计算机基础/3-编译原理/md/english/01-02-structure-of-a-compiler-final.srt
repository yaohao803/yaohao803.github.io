0
00:00:03,169 --> 00:00:06,670
Welcome back, in this second half of the lecture we'll continue with our overview

1
00:00:06,670 --> 00:00:13,670
of the structure of a compiler. Recall that a compiler has five major phases,

2
00:00:16,199 --> 00:00:21,390
lexical analysis, parsing, semantic analysis, optimization, and code

3
00:00:21,390 --> 00:00:25,510
generation. And now we're going to briefly talk about each one, and we're going to

4
00:00:25,510 --> 00:00:30,850
explain how a compiler understands these with an analogy to how humans understand

5
00:00:30,850 --> 00:00:37,850
English. The first step at understanding a program, both for a compiler and for a

6
00:00:38,379 --> 00:00:45,069
human, is to understand the words. Now, humans can look at this example sentence

7
00:00:45,069 --> 00:00:52,069
and immediately recognize that there are four words 'this is a' and 'sentence. And

8
00:00:52,789 --> 00:00:57,449
this is so automatic that you don't even think about it but there is [inaudible]

9
00:00:57,449 --> 00:01:01,209
real computation going on here. You have to recognize the separators, namely the

10
00:01:01,209 --> 00:01:07,759
blanks. And the punctuation, things like the periods, and also clues like capital

11
00:01:07,759 --> 00:01:13,289
letters. And these help you to divide up this group of letters into, a bunch of

12
00:01:13,289 --> 00:01:18,030
words that you can understand. And just to emphasize that this is not completely

13
00:01:18,030 --> 00:01:24,100
trivial, let's take a look at this sentence. And you can read this, but it

14
00:01:24,100 --> 00:01:28,250
takes a little bit of time Because I've put the separators in, in odd places. So

15
00:01:28,250 --> 00:01:34,799
you can see the word is, the word this, the word a, and the word sentence. But

16
00:01:34,799 --> 00:01:38,820
again this isn't something that comes to you immediately. You actually have to do

17
00:01:38,820 --> 00:01:44,060
some work to see where the divisions lie Because they're not given to you in the

18
00:01:44,060 --> 00:01:49,920
way that you're used to. The goal of lexical analysis, then, is to divide the

19
00:01:49,920 --> 00:01:56,259
program text into its words, or what we call in compiler speak, the tokens. So,

20
00:01:56,259 --> 00:02:00,009
here's an, an example piece of program text now, instead of a piece of English

21
00:02:00,009 --> 00:02:05,579
text, and let's walk through this and identify the tokens. So, there's some

22
00:02:05,579 --> 00:02:11,489
obvious ones that are keywords, like if, and then. >> And else that we want to

23
00:02:11,489 --> 00:02:18,489
identify. And then there are variable names, things like X, and Y, and Z.

24
00:02:19,120 --> 00:02:25,519
There's also constants, things like number one, and the number two. And then there

25
00:02:25,519 --> 00:02:32,019
are some operators, double equals is one, and the assignment operator is another.

26
00:02:32,019 --> 00:02:36,260
And here's already an interesting question. How do we know that double

27
00:02:36,260 --> 00:02:41,170
equals is not two individual equals signs? How do we know that we want this? To be a

28
00:02:41,170 --> 00:02:45,909
double equal so we want, and not two single equals. Well, we don't know right

29
00:02:45,909 --> 00:02:50,610
now but we'll talk about that. >> In the lecture on how we implement Lexico

30
00:02:50,610 --> 00:02:53,959
analysis. But we're not done with all the tokens in this example either, there's a

31
00:02:53,959 --> 00:03:00,040
few more. The semi colons, the punctuation are also tokens, and then the separators

32
00:03:00,040 --> 00:03:04,560
are also tokens, so here's a blank, that's a token, here's another blank, that's

33
00:03:04,560 --> 00:03:08,230
another token, and then there are lots of blanks here that serve to separate things

34
00:03:08,230 --> 00:03:12,150
like the keywords and the variable names and other symbols from each other. And

35
00:03:12,150 --> 00:03:19,150
those are the tokens of this example. So for humans, once the words are understood,

36
00:03:19,569 --> 00:03:22,659
the next step is to understand the structure of the sentence, and this is

37
00:03:22,659 --> 00:03:26,879
called parsing. And as we all learned in elementary school, this means diagramming

38
00:03:26,879 --> 00:03:32,110
sentences, and these diagrams are trees, and it's a very simple procedure. Let's

39
00:03:32,110 --> 00:03:36,969
look at this example. This line is a longer sentence. The first step in parsing

40
00:03:36,969 --> 00:03:42,790
is to identify the role of each word in the sentence. So we have things like nouns

41
00:03:42,790 --> 00:03:49,730
and verbs and adjectives. But then, the actual work of parsing is to group these

42
00:03:49,730 --> 00:03:53,860
words together into higher level constructs. So for example, this

43
00:03:53,860 --> 00:04:00,860
particular sentence consists of a subject, a verb, and an object, okay? And that

44
00:04:01,469 --> 00:04:04,719
actually forms an entire sentence. So, right here we have the root of the tree

45
00:04:04,719 --> 00:04:07,989
called a sentence, and that's broken down into constituent parts. The high level

46
00:04:07,989 --> 00:04:11,890
structure, as we said, is subject verb to object. And then the subject is more

47
00:04:11,890 --> 00:04:18,890
complicated, as is the object. And this is an example of parsing an English sentence.

48
00:04:19,940 --> 00:04:24,420
The analogy between parsing English text and parsing program text is very strong.

49
00:04:24,420 --> 00:04:28,510
In fact, they're exactly the same thing. So here's our little example piece of code

50
00:04:28,510 --> 00:04:32,880
again, so let's work through parsing it. So, clearly, this is an if then else

51
00:04:32,880 --> 00:04:38,160
statement, and so, the root of our diagram, of our parse tree, is gonna be if

52
00:04:38,160 --> 00:04:42,350
then else. [inaudible] Nothing else consists of three parts. There's a

53
00:04:42,350 --> 00:04:46,290
predicate, a then statement, and an L statement. And now let?s look at the

54
00:04:46,290 --> 00:04:51,920
predicate which consists of three pieces. There's a variable, a comparison operator

55
00:04:51,920 --> 00:04:57,850
and another variable and together those form a relation. So the comparison between

56
00:04:57,850 --> 00:05:02,610
two things is one of the things you can have as a valid predicate. Similarly, the

57
00:05:02,610 --> 00:05:08,560
then statement consists of an assignment where Z gets one, and the else statement also

58
00:05:08,560 --> 00:05:15,560
has the form of an assignment, Z gets two. And to, all together this is a parse tree,

59
00:05:15,920 --> 00:05:19,320
of the if-then-else, showing its structure, breaking it up into its

60
00:05:19,320 --> 00:05:26,320
constituent pieces. Now, once we've understood the sentence structure, the

61
00:05:26,600 --> 00:05:30,930
next step is to try to understand the meaning, of what has been written. And,

62
00:05:30,930 --> 00:05:36,180
this is hard. So, actually we don't know how this works for humans still. We don't

63
00:05:36,180 --> 00:05:40,850
understand, what happens after lexical analysis and parsing. We do know that

64
00:05:40,850 --> 00:05:46,280
people do lexical analysis and parsing in much the same way, that compilers

65
00:05:46,280 --> 00:05:50,910
lexically analyze and parse programs. But frankly, understanding meaning is

66
00:05:50,910 --> 00:05:55,830
something that is simply too hard for compilers. So, the first important thing

67
00:05:55,830 --> 00:06:01,710
to understand about, semantic analysis is that compilers can only do very limited

68
00:06:01,710 --> 00:06:06,490
kinds of semantic analysis. And in particular the kinds of things that

69
00:06:06,490 --> 00:06:10,870
compilers generally do are try to catch inconsistencies. So, if the program is

70
00:06:10,870 --> 00:06:15,930
somehow self inconsistent, [inaudible] compilers can often notice that and report

71
00:06:15,930 --> 00:06:22,130
errors. But they don't really know what the program is supposed to do. As an

72
00:06:22,130 --> 00:06:27,590
example of the kind of thing that we do in semantic analysis, again, using an analogy

73
00:06:27,590 --> 00:06:32,180
in English, let's consider the following sentence. So, Jack said Jerry left his

74
00:06:32,180 --> 00:06:37,930
assignment at home. And the question is, what, who does his, refer to here? It

75
00:06:37,930 --> 00:06:42,580
could be that his refers to Jerry, in which case we would read, Jack said Jerry

76
00:06:42,580 --> 00:06:48,540
left Jerry's assignment at home. Or it could refer to Jack. In which case, we

77
00:06:48,540 --> 00:06:52,630
could read the sentence as, Jack said Jerry left Jack's assignment at home. And

78
00:06:52,630 --> 00:06:58,450
without more information, we actually don't know which one. His is referring to,

79
00:06:58,450 --> 00:07:02,980
whether it's Jack, or it's Jerry. And even worse, let's take a look at this sentence

80
00:07:02,980 --> 00:07:07,700
down here. Jack said, Jack left his assignment at home. And the question is

81
00:07:07,700 --> 00:07:14,700
how many people are actually involved in this sentence? It could be as many as

82
00:07:14,860 --> 00:07:19,280
three, there could be two separate Jacks and his, could even refer to somebody

83
00:07:19,280 --> 00:07:23,450
completely different. We don't know without seeing the rest of the story. That

84
00:07:23,450 --> 00:07:27,910
surrounds this sentence, all the possibilities for his. But it could also

85
00:07:27,910 --> 00:07:32,740
be as few as, only a single person. It could be that Jack and Jack and his are

86
00:07:32,740 --> 00:07:37,700
all the same person in this sentence. And so this kind of ambiguity is a real

87
00:07:37,700 --> 00:07:44,700
problem, in semantic analysis. And the analogy in programming languages is

88
00:07:46,250 --> 00:07:50,720
variable bindings. So we would have variables, in this case, a variable called

89
00:07:50,720 --> 00:07:54,340
Jack or maybe more than one variable called Jack. And a programming language is

90
00:07:54,340 --> 00:08:01,210
going to have very strict rules to prevent the kind of ambiguities we had in the

91
00:08:01,210 --> 00:08:06,900
English sentences on the previous slide. So you know, in this example. Question is

92
00:08:06,900 --> 00:08:12,180
what value is printed by this output statement, and the answer is it's going to

93
00:08:12,180 --> 00:08:18,110
print four because this use of the variable Jack binds to this definition

94
00:08:18,110 --> 00:08:23,970
here. And the outer definition is hidden. So the outer definition is not active in

95
00:08:23,970 --> 00:08:27,390
this scope because it is hidden by the inner definition and that is just a

96
00:08:27,390 --> 00:08:34,390
standard rule of a lot of lexically scoped programming languages. Now the pilots

97
00:08:34,990 --> 00:08:39,300
perform many semantic texts besides analyzing the variable bindings. And so

98
00:08:39,299 --> 00:08:45,829
here's another example in English. So Jack looked her homework at home. And, under

99
00:08:45,829 --> 00:08:49,949
the usual naming conventions, assuming that Jack is male, we know there's a type

100
00:08:49,949 --> 00:08:56,050
mismatch between Jack and her. So we know that, whatever her is, it is not Jack.

101
00:08:56,050 --> 00:08:59,610
And, and therefore we known that this sentence is talking about two different

102
00:08:59,610 --> 00:09:06,569
people. And so this is, analogous to type checking. The fourth compiler phase,

103
00:09:06,569 --> 00:09:10,959
optimization, doesn't have a very strong counterpart in everyday English usage but

104
00:09:10,959 --> 00:09:14,480
it's a little bit like editing. And, in fact, it's a lot like what professional

105
00:09:14,480 --> 00:09:18,779
editors do when they have to reduce the length of an article to get it down to

106
00:09:18,779 --> 00:09:23,889
some word budget. So, for example, I have this phrase right here, but a little bit

107
00:09:23,889 --> 00:09:29,060
like ending; and if I didn't like it, if I thought it was too long, I could replace

108
00:09:29,060 --> 00:09:36,060
the middle four words, with two words. Akin to. So now it says, but akin to

109
00:09:36,290 --> 00:09:40,250
editing, and that means exactly the same thing as the original phrase, but uses

110
00:09:40,250 --> 00:09:47,250
fewer words. And the goal in program optimization Is to modify the program so

111
00:09:47,279 --> 00:09:51,180
that it uses less of some resource. Maybe we want to use less time, we want the

112
00:09:51,180 --> 00:09:57,730
program to run faster maybe we want it to use less space so that we can fit more

113
00:09:57,730 --> 00:10:02,329
data in memory. For a handheld device we might be interested in reducing the amount

114
00:10:02,329 --> 00:10:06,910
of power that it uses. If we have external communication we might be interested in

115
00:10:06,910 --> 00:10:12,220
reducing the number of network messages or the number of database accesses. And

116
00:10:12,220 --> 00:10:17,800
there's any number of resources that we might want to improve other program's use

117
00:10:17,800 --> 00:10:24,800
of. So here's a simple example of the kinds of optimizations a program might do.

118
00:10:28,019 --> 00:10:32,600
We can have a rule in our compiler that says X equals Y times zero, is the same as

119
00:10:32,600 --> 00:10:36,769
X equals zero. And this seems like a real improvement, because instead of doing the

120
00:10:36,769 --> 00:10:42,449
multiply, we can just do an assignment. So we save some computation by doing that.

121
00:10:42,449 --> 00:10:47,670
Now unfortunately this is not a correct rule. And this is one of the important

122
00:10:47,670 --> 00:10:51,449
things to know about compiling optimization, is that it's not always

123
00:10:51,449 --> 00:10:57,249
obvious when it's legal to do certain optimizations or not. Now it turns out

124
00:10:57,249 --> 00:11:04,249
That this particular rule is valid for integers. Okay, so if X and Y are

125
00:11:04,819 --> 00:11:10,379
integers, then multiplying by zero is always the same thing as just signing

126
00:11:10,379 --> 00:11:17,379
zero. But, it's invalid for floating point. And why is that, well because you

127
00:11:20,350 --> 00:11:24,540
have to know some details of the IEEE floating point standard, but there is a

128
00:11:24,540 --> 00:11:29,550
special number in the IEEE standard called not a number and it turns out that

129
00:11:29,550 --> 00:11:36,550
not a number called a NaN times zero is equal to not a number. Any particular

130
00:11:37,610 --> 00:11:43,540
non-number times zero is not equal to zero If X and Y are plotting point numbers, you

131
00:11:43,540 --> 00:11:47,790
can't do this optimization. In fact, if you did this optimization, it would break

132
00:11:47,790 --> 00:11:52,470
certain very important algorithms that rely on the proper propagation of

133
00:11:52,470 --> 00:11:59,470
not a number. Finally, the last compiler phase is code generation, often

134
00:12:00,689 --> 00:12:07,040
referred to as Code Gen, and Code Gen, can produce assembly code. That's the most

135
00:12:07,040 --> 00:12:10,579
common thing that a compiler would produce. But in general, it's a

136
00:12:10,579 --> 00:12:15,040
translation into some other language. And this is, entirely analogous to human

137
00:12:15,040 --> 00:12:21,499
translation. So just as a human translator might translate, English into French, a

138
00:12:21,499 --> 00:12:28,499
compiler will translate a high level program into assembly code. To wrap up,

139
00:12:28,680 --> 00:12:34,360
almost every compiler has the five phases that we outlined. However, the proportions

140
00:12:34,360 --> 00:12:38,899
have changed a lot over the years, and if we were to go back to FORTRAN I and

141
00:12:38,899 --> 00:12:43,809
look inside of that compiler, we would probably see a size and complexity that

142
00:12:43,809 --> 00:12:49,550
looks something like this. We have a fairly complex lexical analysis phase, an

143
00:12:49,550 --> 00:12:56,550
equally complicated parsing phase, a very small semantic analysis phase, a. A fairly

144
00:12:57,199 --> 00:13:03,980
involved optimization phase and another fairly involved code generation phase. And

145
00:13:03,980 --> 00:13:10,259
so we see a compiler where the complexity was sp, spread fairly evenly throughout

146
00:13:10,259 --> 00:13:15,199
except for its semantic analysis which is very weak in the early days. And today

147
00:13:15,199 --> 00:13:19,749
if we look at a modern compiler you'll see almost nothing in lengthening, very little

148
00:13:19,749 --> 00:13:25,269
in parsing, because we have extremely good tools to help us write those two phases.

149
00:13:25,269 --> 00:13:31,569
We would see a fairly involved thematic analysis phase. We would see a very large

150
00:13:31,569 --> 00:13:36,930
optimization phase, and this is in fact the dominant component off all modern

151
00:13:36,930 --> 00:13:43,290
compilers, and the a small code-generation phase because again we understand that

152
00:13:43,290 --> 00:13:48,949
phase very, very well. That's it for this lecture. Future lectures, we'll look at

153
00:13:48,949 --> 00:13:50,580
each of these phases in detail.
